{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive summary\n",
    "\n",
    "This the purpose of this project is to determine what factors contribute most to sales for a national quick service restaurant and how we, as a digital marketing firm, can use those factors to more efficiently determine where we should focus a campaign. Specifically, we will examine what factors increase online sales as measured by conversions. Conversions are when an online sale is made through an advertizement. \n",
    "\n",
    "Many factors have been linked to sales, but the interaction of these factors has not been determined. The goal of this study is to determine the relationship between several of these factors in order to maximize return on ad spend. Weather is an important factor in the online sales of this company as there are much greater sales during winter months. There are also sales spikes around holidays, the end of the week, and big sporting events(mainly football). If we can show which combination of these factors predict high sales, we can use different thresholds of these factors to determine where we want to be spending our advertising dollar.  \n",
    "\n",
    "Through looking at past data, we can determine what the most important predictors of higher than normal online sales might be. We will use an ensemble decision tree method called random forest in order to both predict values for online sales as well as classifying whether a day will be a low, medium low, medium high, or high conversion day. If we know that a day might be a high conversion day anyways, we could spend less money on those days, and focus resources on days where there is more potential to increase conversions.\n",
    "\n",
    "We used 2014 and 2015 data to see if we could produce a good model of one year based on the last years data. If the 2014 is a good predictor of the conversions in 2015, we can apply this model to determine our targeted ad spend in 2016. The random forest models we have used are very good at classifying days as mentioned above and will provide a good basis for what type day we think we will see. \n",
    "\n",
    "We saw some inconsistencies in the 2015 data, which a had much lower conversion average (50% lower) and much lower maximum values. We therefore focused on using the 2014 data to build the strongest model we could. To do this we hid the actual conversion values for 2014 from the model and asked it to predict the number of conversions.  We were able to produce a model that predicted values that were very correlated to the known actual values for those days. However, there was a lot of variation in the predictions, meaning that there were sometimes predicted spikes when there were none in the actual data and vice versa. We decided to take another approach that would give us actionable data without worrying about incorrect spikes in conversions.\n",
    "\n",
    "We broke the conversions up into high, high medium, low medium and low conversion days base on quartiles of the data. we then asked the model to again place the different days into these categories without knowing the class the actual conversions belonged in. This model was far more successful and produced actionable predictions. This model correctly predicts high sales days more than 90% of the time. It also exceeded at predicting the other categories of conversions. We can continue to use this model to more efficiently use our ad spend to maximize return. We can avoid spending too much on days we would have high conversions regardless and focus on boosting conversions on days where we would predict lower conversions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Review and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and data cleaning\n",
    "The football games that are included in the analysis are all NFL games (including playoffs) and college football games that involve a top 25 ranked team. This data was queried and downloaded as a csv from sportsreference.com. Weather data was gathered through a query on the NOAA website, which included data from 3 weather stations covering the metro Atlanta area, giving a good overview of the weather trends. All data was gathered by date, so it will be easy to join dataframes and be confident in the data being concurrent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.cross_validation import KFold,cross_val_score,train_test_split,cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize, LabelEncoder\n",
    "from sklearn.metrics import r2_score,classification_report,roc_curve,auc,accuracy_score,precision_score,recall_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from scipy import interp\n",
    "import scipy\n",
    "import psycopg2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing nfl 2014 schedule and results data set, downloaded from sportsreference.com\n",
    "nfl14 = pd.read_csv('../Assets/nfl2014schedule.csv') \n",
    "nfl14['Date'] = '2014 ' + nfl14['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing nfl 2015 schedule and results data set, downloaded from sportsreference.com\n",
    "\n",
    "nfl15 = pd.read_csv('../Assets/nfl2015schedule.csv') \n",
    "nfl15['Date'] = '2015 ' + nfl15['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#joining the dataframes\n",
    "nflSched = pd.concat([nfl14,nfl15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dropping columns that won't help predict sales\n",
    "nflSched.drop(['Week','Day','Unnamed: 4','Unnamed: 6','PtsW','PtsL','YdsW','TOW','YdsL','TOL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eliminating strings that were found in empty rows in the dataframe\n",
    "nflSched = nflSched[nflSched['Date']!= '2014 Date']\n",
    "nflSched = nflSched[nflSched['Date']!= '2014 Playoffs']\n",
    "nflSched = nflSched[nflSched['Date']!= '2015 Date']\n",
    "nflSched = nflSched[nflSched['Date']!= '2015 Playoffs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting to datetime and setting as index\n",
    "nflSched['Date'] = pd.to_datetime(nflSched['Date'], format='%Y %B %d')\n",
    "nflSched.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a column so that we will have a column that indicates dates that have an NFL game\n",
    "nflSched['NflGame'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing college football 2014 and 2015 schedule and results data set, downloaded from sportsreference.com\n",
    "cfb14 = pd.read_csv('../Assets/cfb2014schedule.csv') \n",
    "cfb15 = pd.read_csv('../Assets/cfb2015schedule.csv')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combining the dataframes\n",
    "cfbSched = pd.concat([cfb14,cfb15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping columns that I don't think will be good predictors\n",
    "cfbSched.drop(['Rk','Wk','Time','Day','Pts','Unnamed: 7','Pts.1','Notes'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing columns that just contain the string 'Date'\n",
    "cfbSched = cfbSched[cfbSched['Date']!= 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting to datetime and setting as index\n",
    "cfbSched['Date'] = pd.to_datetime(cfbSched['Date'], format='%b %d %Y')\n",
    "cfbSched.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a column so that we will have a column that indicates dates that have an cfb game\n",
    "cfbSched['cfbGame'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making better column names to see if network, or teams involved influence sales\n",
    "cfbSched=cfbSched.rename(columns = {'Winner/Tie':'CfbWinner', 'Loser/Tie':'CfbLoser', 'TV':'CfbTV'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing weather data from the noaa\n",
    "Weather = pd.read_csv('../Assets/AtlWeather.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting to datetime\n",
    "Weather['DATE'] = pd.to_datetime(Weather['DATE'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping columns we don't need, and eliminating null value place holders\n",
    "Weather.drop([u'STATION',u'STATION_NAME',u'MDPR',u'DAPR',u'SNWD',u'TOBS',u'WESD'\n",
    "             , u'WESF', u'WT01', u'WT06', u'WT02', u'WT04', u'WT08', u'WT03','PSUN','TAVG','SNOW','TSUN'], axis=1, inplace=True)\n",
    "\n",
    "Weather.replace('-9999',np.nan, inplace=True)\n",
    "Weather.replace('-9999.0',np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Weather.rename(columns = {'DATE':'Date'}, inplace=True)\n",
    "Weather.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#There are many weather stations across the Atlanta area, so we are taking the mean of these stations to estimate weather for\n",
    "#the metro area\n",
    "Weather = Weather.groupby(Weather.index).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final = pd.merge(Weather, cfbSched, left_index=True, right_index=True, how='outer')\n",
    "Final = pd.merge(Final, nflSched, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#slicing the dataframe to include the pro and college football seasons from 2014 to 2016 for a master table\n",
    "Final = Final.ix['2014-8':'2016-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../Assets/AtlantaSales2014_2016.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1ccb2a5c3db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#importing client data for Atlanta area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PSA Sales'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Orders'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Online Sls %'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Carryout %'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Rewards Enrollment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Assets/AtlantaSales2014_2016.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jb3/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jb3/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jb3/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jb3/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jb3/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3246)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6111)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../Assets/AtlantaSales2014_2016.csv does not exist"
     ]
    }
   ],
   "source": [
    "#importing client data for Atlanta area \n",
    "columns=['Date','PSA Sales','Orders','Online Sls %', 'Carryout %', 'Rewards Enrollment']\n",
    "conv = pd.read_csv('../Assets/AtlantaSales2014_2016.csv', names=columns, header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv['Date'] = pd.to_datetime(conv['Date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in conv.columns:\n",
    "    if conv[i].dtype == object:\n",
    "        conv[i] = conv[i].convert_objects(convert_numeric=True)\n",
    "\n",
    "conv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv.rename(columns = {'PSA Sales':'psa_sales','Online Sls %':'Online%','Carryout %':'carryout%',\n",
    "                         'Papa Rewards Customer Enrollments':'rewards'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv['online_sales'] = conv['psa_sales'] * (conv['Online%']/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merging the dataframe with the final dataframe\n",
    "Final = pd.merge(Final, conv, left_index=True, right_index=True, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#slicing the dataframe to include the pro and college football seasons from 2014 to 2015 so that we can look at the \n",
    "#seasons separately \n",
    "Final2 = Final.ix['2014-8':'2015-3']\n",
    "Test = pd.DataFrame()\n",
    "Test = Final.ix['2015-8':'2016-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replacin NaN's with zeros so that we can identify whether there was or was not a football game \n",
    "Final2['cfbGame'].replace('NaN', 0.0,inplace=True)\n",
    "Final2['NflGame'].replace('NaN', 0.0, inplace=True)\n",
    "\n",
    "Test['cfbGame'].replace('NaN', 0.0,inplace=True)\n",
    "Test['NflGame'].replace('NaN', 0.0, inplace=True)\n",
    "#1=game on that day, 0=no game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final2['Day']=Final2.index.weekday\n",
    "Test['Day']=Test.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #establishing connection to SQL database, dumping final table into database\n",
    "# user = \"postgres:Lumberjack1\"\n",
    "# engine = sqlalchemy.create_engine('postgresql://{}{}'.format(user,'@localhost:5432/Rforest'))\n",
    "# Final.to_sql(\"final\", con = engine, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "# Final2.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Final2.drop('CfbWinner','CfbLoser','CfbTV','Winner/tie','Loser/tie')\n",
    "Final2.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "|Column Name|Data|\n",
    "|-----------|----|\n",
    "|PRCP| The amount of precipitation that day in inches|\n",
    "|TMAX| The maximum measured temperature for the day in degrees Farenheit|\n",
    "|TMIN| The minimum measured temperature for the day in degrees Farenheit|\n",
    "|CfbWinner| The team that won the college football game on that day|\n",
    "|CfbLoser| The team that lost the college football game on that day|\n",
    "|CfbTV| Television network a college football game was broadcast on|\n",
    "|cfbGame| Whether there was a college football game on that day|\n",
    "|Winner/tie| The team that won the NFL game on that day|\n",
    "|Loser/tie| The team that lost the NFL game on that day|\n",
    "|NflGame| Whether there was an NFL game on that day|\n",
    "|psa_sales| per store average sales|\n",
    "|Orders| transaction volume for Atlanta market|\n",
    "|online%| percentage of per store average sales that were made online|\n",
    "|carryout%| percentage of orders that were carryout|\n",
    "|rewards| Number of rewards program members|\n",
    "|online_sales| psa_sales multiplied by percentage of online sales|\n",
    "|Day| day of the week represented by integers 0-6 with 0 being Monday|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot('online_sales','Day', Final2)\n",
    "\n",
    "plt.figure(2)\n",
    "sns.lmplot('online_sales','NflGame',Final2, logistic=True)\n",
    "\n",
    "plt.figure(3)\n",
    "sns.lmplot('online_sales','cfbGame', Final2, logistic=True)\n",
    "\n",
    "plt.figure(4)\n",
    "sns.lmplot('online_sales','TMAX', Final2)\n",
    "\n",
    "plt.figure(5)\n",
    "sns.lmplot(x='online_sales',y='PRCP',data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot(Final2['Orders'],Final2['PRCP'])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot(Final2['Orders'],Final2['TMAX'])\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot(Final2['Orders'],Final2['cfbGame'])\n",
    "\n",
    "plt.figure(4)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot(Final2['Orders'],Final2['NflGame'])\n",
    "\n",
    "plt.figure(5)\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.regplot(Final2['Orders'],Final2['Day'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using category codes for columns with many categorical variables (winners/losers of games, tv network, etc)\n",
    "for i in Final2.columns:\n",
    "    if Final2[i].dtype == object:\n",
    "        Final2[i] = Final2[i].astype('category')\n",
    "        Final2[i] = Final2[i].cat.codes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in Test.columns:\n",
    "    if Test[i].dtype == object:\n",
    "        Test[i] = Test[i].astype('category')\n",
    "        Test[i] = Test[i].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final2.dropna(inplace=True)\n",
    "print Final2.shape\n",
    "\n",
    "Test.dropna(inplace=True)\n",
    "print Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating target and predictors for 2014 data to see if these are good predictors for conversions before I use 2014 data \n",
    "#to try to predict 2015 data\n",
    "y=Final2['online_sales']\n",
    "X=Final2.drop(Final2.ix[:,10:], axis=1)\n",
    "X['Day']=Final2['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting a decision tree model using cross validation\n",
    "cv = KFold(len(y_train), shuffle=False) \n",
    "print cv\n",
    "dt = DecisionTreeRegressor(random_state=5)\n",
    "dtScore = cross_val_score(dt, X_train, y_train, cv=cv,n_jobs=1)\n",
    "print \"Regular Decision Tree scores are:\", dtScore\n",
    "print \"Regular Decision Tree average score is:\", dtScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting a random forest model using cross validation and comparing it to previous model\n",
    "\n",
    "rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "           min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=200, n_jobs=-1, oob_score=False, random_state=5,\n",
    "           verbose=0, warm_start=False)\n",
    "rfScore = cross_val_score(rf, X_train,y_train, cv=cv, n_jobs=1)\n",
    "print \"Random Forest scores are:\", rfScore\n",
    "print \"Regular Decision Tree scores are:\", dtScore\n",
    "print \"Random Forest average score is:\", rfScore.mean()\n",
    "print \"Regular Decision Tree average score is:\", dtScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rfc = RandomForestRegressor(n_jobs=-1, max_features= 'sqrt' ,n_estimators=100) \n",
    "\n",
    "# param_grid = { \n",
    "#     'n_estimators': [100,200,300,400,500],\n",
    "#     'max_features': [None, 'sqrt', 'log2'],\n",
    "#     'min_samples_split':[1,2,3,4,5,6]\n",
    "# }\n",
    "\n",
    "# CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "# CV_rfc.fit(X_train, y_train)\n",
    "# print CV_rfc.best_params_\n",
    "# print CV_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting a adaboost model using cross validation and comparing it to previous model\n",
    "\n",
    "ab = AdaBoostRegressor(base_estimator=None, learning_rate=2.0, loss='linear',\n",
    "         n_estimators=300, random_state=5)\n",
    "abScore = cross_val_score(ab, X_train,y_train, cv=cv, n_jobs=1)\n",
    "print \"Adaptive Boost scores are :\",abScore\n",
    "print \"Random Forest scores are:\", rfScore\n",
    "print \"Regular Decision Tree scores are:\", dtScore\n",
    "print \"Adaptive Boost average score is:\",abScore.mean()\n",
    "print \"Random Forest average score is:\", rfScore.mean()\n",
    "print \"Regular Decision Tree average score is:\", dtScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grid search for best parameters\n",
    "# abc = AdaBoostRegressor() \n",
    "\n",
    "# param_grid2 = { \n",
    "#     'n_estimators': [50,100,150,200],\n",
    "#     'learning_rate': [1.0,2.0,3.0,4.0]\n",
    "# }\n",
    "\n",
    "# CV_abc= GridSearchCV(estimator=abc, param_grid=param_grid2, cv= 5)\n",
    "# CV_abc.fit(X_train, y_train)\n",
    "# print CV_abc.best_params_\n",
    "# print CV_abc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting cross-validated models\n",
    "def do_plot(model, m=None):\n",
    "    for fold, color in zip(cv, ['r','g','b']):#colors are from different folds from Kfold, so 3 diff models\n",
    "        \n",
    "        X_train = X.iloc[fold[0]]\n",
    "        X_test =  X.iloc[fold[1]]\n",
    "        y_train = y.iloc[fold[0]]\n",
    "        y_test = y.iloc[fold[1]]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        plt.scatter(y_test, y_pred, color=color)\n",
    "        plt.plot([0,3500],[0,3500])\n",
    "        plt.text(2500,0, \"R2:\"+str(m), fontsize=20, )\n",
    "        plt.xlabel('Actual Conversions')\n",
    "        plt.ylabel('Predicted Conversions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_plot(dt, dtScore.mean().round(2))\n",
    "plt.title(\"Regular Decision Trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_plot(rf, rfScore.mean().round(2))\n",
    "plt.title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_plot(ab, abScore.mean().round(2))\n",
    "plt.title(\"Adaptive Boost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating feature importance for random forest\n",
    "all(rf.feature_importances_ == np.mean([tree.feature_importances_ for tree in rf.estimators_], axis=0))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Random Forest Feature importances\", fontsize = 30)\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90, fontsize = 20)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating feature importance for Adaboost\n",
    "all(ab.feature_importances_ == np.mean([tree.feature_importances_ for tree in ab.estimators_], axis=0))\n",
    "\n",
    "importances2 = ab.feature_importances_\n",
    "std2 = np.std([tree.feature_importances_ for tree in ab.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances)[::-1]\n",
    "feature_names2 = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Adaboost feature importances\", fontsize = 30)\n",
    "plt.bar(range(X_train.shape[1]), importances2[indices],\n",
    "       color=\"r\", yerr=std2[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_names2[indices], rotation=90, fontsize = 20)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in both the adaptive boost model and the randome forest model, weather and day of the week seem to be more important features for splitting the data and sporting events are less important. This could lead to better targeting of ads, where the conversions will happen regardless of whether there is a sports game on, but we can increase ads on days where the weather is not indicative of great sales in order to increase sales on those days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#random forest predictions\n",
    "rfy_pred = rf.predict(X_test)\n",
    "rfpredictions = pd.DataFrame()\n",
    "rfpredictions['actual'] = y_test\n",
    "rfpredictions['predict'] = rfy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test train split of model on 2014 data\n",
    "sns.regplot(rfpredictions['actual'],rfpredictions['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_score(rfpredictions['actual'],rfpredictions['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfpredictions.plot()\n",
    "plt.ylabel('Conversions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfpredictions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used several different models to see which one performs the best on this split data. The random forest performed best and was applied to the 2014 data to produce predictions. The model has a good r2_score at 0.86, but we can see from the plot of the predicted vs. actual values that there is a lot of variation away from the known values, with some big predicted spikes where there are none in the real data. For this reason we will later perform a random forest classifier to try to bin the conversions and make for less variation in the predicted classes as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now fit model to all of 2014 data and predict 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conv2015 = pd.DataFrame()\n",
    "Conv2015['actual'] = Test['online_sales']\n",
    "Features15 = Test.drop(Test.ix[:,10:], axis=1)\n",
    "X2=Final2.drop(Final2.ix[:,10:], axis=1)\n",
    "y2=Final2['online_sales']\n",
    "X2['Day']=Final2['Day']\n",
    "Features15['Day']=Test['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fitting model to all of the 2014 data\n",
    "rf.fit(X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred15 = rf.predict(Features15)\n",
    "Conv2015['predicted'] = pred15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(Conv2015['actual'],Conv2015['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_score(Conv2015['actual'],Conv2015['predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression model for 2015 data is not very good. The r2_score is around 0.5, which means that we have a positive correlation, but not a very strong one.  This is perhaps a situation where a classification model may be a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conv2015.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conv2015.plot()\n",
    "plt.ylabel('Conversions')\n",
    "plt.ylim(0, 3500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adapdtive boost and the regular decision tree regression models did not do as well as a random forest at predicting online sales in a given day after being trained on the split data for 2014 (70% train, 30% test). The r^2 for the Adaboost and the regular decision tree were greater then 0.10 less than the random forest, which did much better. \n",
    "\n",
    "The random forest regression model was good at predicting within year, but not very good at using results from one year to predict another. The r^2 for the 2014 only random forest regression model was 0.86, which is a good score, not great, but good enough to give some valuable insights. We therefore decided to proceed with just the random forest model. When the model was fit to the 2014 data and used to predict 2015 values the r^2 was only slightly above 0.5, making it not a great model to predict actual sales values.\n",
    "\n",
    "If we decide to use a random forest classifier with 4 different categories of online sales, we may be able to make predictions for where to spend ad money based on a more general classification rather than a specific online sales value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final2['online_sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaking up conversions into quartiles for classification\n",
    "def classConversions(cl):\n",
    "    if cl > 2271: \n",
    "        return 3\n",
    "    elif 1990 < cl <= 2271:\n",
    "        return 2\n",
    "    elif 1620 < cl <= 1990:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adding classifications to the dataframe\n",
    "Final2['Class'] = Final2['online_sales'].map(classConversions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test['Class'] = Test['online_sales'].map(classConversions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','cfbGame', data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','TMIN', data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','PRCP', data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','NflGame', data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','TMAX', data=Final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot('Class','Day', data=Final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These violin plots show the data in a different manner and allow us to see what the features are like when we see most of our high online_sales days. These plots indicate that sometimes there is an increase in high sales days when it rains more than 0.5 inches in a day. Also we can see that almost all high sales days occur on Saturdays. High sales also occur more often when the minimum temperature is between 80-100 and 40-60 degrees. These plots help, by showing us some possible thresholds for predicting high sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using classifier to predict 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1=Final2['Class']\n",
    "X1=Final2.drop(Final2.ix[:,10:], axis=1)\n",
    "X1['Day']=Final2['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.30, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Grid search cv for parameters\n",
    "# rfc1 = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=100, oob_score = True) \n",
    "\n",
    "# param_grid1 = { \n",
    "#     'n_estimators': [100,200,300,400,500],\n",
    "#     'criterion': [\"gini\"],\n",
    "#     'max_features': [None, 'sqrt', 'log2'],\n",
    "#     'min_samples_split':[1,2,3,4,5,6]\n",
    "# }\n",
    "\n",
    "# CV_rfc1 = GridSearchCV(estimator=rfc1, param_grid=param_grid1, cv= 5)\n",
    "# CV_rfc1.fit(X_train1, y_train1)\n",
    "# print CV_rfc1.best_params_\n",
    "# print CV_rfc1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_class = KFold(len(y_train1), shuffle=False) \n",
    "print cv_class\n",
    "rfclass = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=4,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "            oob_score=True, random_state=5, verbose=0, warm_start=False)\n",
    "rfclassScore = cross_val_score(rfclass, X_train1, y_train1, cv=cv_class,n_jobs=1)\n",
    "print \"Random forest classifier scores are:\", rfclassScore\n",
    "print \"Regular forest classifier average score is:\", rfclassScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfclass.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all(rfclass.feature_importances_ == np.mean([tree.feature_importances_ for tree in rfclass.estimators_], axis=0))\n",
    "\n",
    "importancesClass = rfclass.feature_importances_\n",
    "stdClass = np.std([tree.feature_importances_ for tree in rfclass.estimators_], axis=0)\n",
    "indicesClass = np.argsort(importancesClass)[::-1]\n",
    "feature_namesClass = X_train1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Random Forest Classifier Feature importances\", fontsize = 30)\n",
    "plt.bar(range(X_train1.shape[1]), importancesClass[indicesClass],\n",
    "       color=\"r\", yerr=stdClass[indicesClass], align=\"center\")\n",
    "plt.xticks(range(X_train1.shape[1]), feature_names[indicesClass], rotation=90, fontsize = 20)\n",
    "plt.xlim([-1, X_train1.shape[1]])\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that precipitation is a more important feature in the classifier model vs. the regression model. It is also interesting that who the winner or loser of a football game is matters more than only whether there is a game or not. Weather and day of the week seem to be the mest predictors of whether a particular day will be a high sales day. These could be useful metrics to use to predict conversions of future data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Class_pred = rfclass.predict(X_test1)\n",
    "Class_predict = pd.DataFrame()\n",
    "Class_predict['actual'] = y_test1\n",
    "Class_predict['predict'] = Class_pred\n",
    "Class_probs = rfclass.predict_proba(X_test)\n",
    "Class_predict['ProbBottom'],Class_predict['ProbMidLow'],Class_predict['ProbMidHi'],Class_predict['ProbHi'] = zip(*Class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Class_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_mat = pd.crosstab(Class_predict['actual'], Class_predict['predict'], rownames=['actual'])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_test1, Class_pred)\n",
    "\n",
    "Scores=pd.DataFrame()\n",
    "Scores['Class'] = ['Low','LowMed','HiMed','Hi']\n",
    "Scores['precision'] = precision\n",
    "Scores['recall'] = recall\n",
    "Scores['fscore'] = fscore\n",
    "Scores['support'] = support\n",
    "Scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes=[0,1,2,3]\n",
    "y_testBi = label_binarize(y_test1, classes)\n",
    "y_predBi = label_binarize(Class_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes=len(classes)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_testBi[:,i], y_predBi[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for each class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Class15 = Test.drop(Test.ix[:,10:], axis=1)\n",
    "X3=Final2.drop(Final2.ix[:,10:], axis=1)\n",
    "y3=Final2['Class']\n",
    "X3['Day']=Final2['Day']\n",
    "Class15['Day']=Test['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfclass.fit(X3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Class15_pred = rfclass.predict(Class15)\n",
    "Class15_predict = pd.DataFrame()\n",
    "Class15_predict['actual'] = Test['Class']\n",
    "Class15_predict['predict'] = Class15_pred\n",
    "Class15_probs = rfclass.predict_proba(Class15)\n",
    "Class15_predict['ProbBottom'],Class15_predict['ProbMidLow'],Class15_predict['ProbMidHi'],Class15_predict['ProbHi'] = zip(*Class15_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Class15_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_mat = pd.crosstab(Class15_predict['actual'], Class15_predict['predict'], rownames=['actual'])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision15, recall15, fscore15, support15 = score(Class15_predict['actual'], Class15_pred)\n",
    "\n",
    "Scores15=pd.DataFrame()\n",
    "Scores15['Class'] = ['Low','LowMed','HiMed','Hi']\n",
    "Scores15['precision'] = precision15\n",
    "Scores15['recall'] = recall15\n",
    "Scores15['fscore'] = fscore15\n",
    "Scores15['support'] = support15\n",
    "Scores15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use threshold to try to improve model, try regular decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write summary of this crappy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_testBi1 = label_binarize(Test['Class'], classes)\n",
    "y_predBi1 = label_binarize(Class15_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes=y_testBi1.shape[1]\n",
    "fpr1 = dict()\n",
    "tpr1 = dict()\n",
    "roc_auc1 = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr1[i], tpr1[i], _ = roc_curve(y_testBi1[:,i], y_predBi1[:,i])\n",
    "    roc_auc1[i] = auc(fpr1[i], tpr1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr1[i], tpr1[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc1[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for each class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This model is not good for predicting classes in the upper two quartiles of sales. However, almost all of the mis-classified points for the class 2 and 3 predictions were in the upper two quartiles. Perhaps we could use a more simple classification method and just classify days as high or low selling days.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryConversions(cl):\n",
    "    if cl > 1990: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test['BiClass'] = Test['online_sales'].map(binaryConversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final2['BiClass'] = Final2['online_sales'].map(binaryConversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Class15Bi = Test.drop(Test.ix[:,10:], axis=1)\n",
    "X4=Final2.drop(Final2.ix[:,10:], axis=1)\n",
    "y4=Final2['BiClass']\n",
    "X4['Day']=Final2['Day']\n",
    "Class15Bi['Day']=Test['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_classBi = KFold(len(y4), shuffle=False) \n",
    "print cv_classBi\n",
    "rfclassBi = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=4,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "            oob_score=True, random_state=5, verbose=0, warm_start=False)\n",
    "rfclassBiScore = cross_val_score(rfclassBi, X4, y4, cv=cv_class,n_jobs=1)\n",
    "print \"Random forest classifier scores are:\", rfclassBiScore\n",
    "print \"Regular forest classifier average score is:\", rfclassBiScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfclassBi.fit(X4,y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Class15Bi_pred = rfclassBi.predict(Class15Bi)\n",
    "Class15Bi_predict = pd.DataFrame()\n",
    "Class15Bi_predict['actual'] = Test['BiClass']\n",
    "Class15Bi_predict['predict'] = Class15Bi_pred\n",
    "Class15Bi_probs = rfclassBi.predict_proba(Class15Bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_mat = pd.crosstab(Class15Bi_predict['actual'], Class15Bi_predict['predict'], rownames=['actual'])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final2.to_csv('../Assets/2014data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.to_csv('../Assets/2015data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [Final2, Test]\n",
    "Tableau = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tableau.to_csv('../Assets/TableauData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grid search cv for parameters\n",
    "DTBinary = DecisionTreeClassifier(max_features= 'sqrt' ) \n",
    "\n",
    "param_grid1 = { \n",
    "    'criterion': [\"gini\"],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'min_samples_split':[1,2,3,4,5,6]\n",
    "}\n",
    "\n",
    "CV_DT = GridSearchCV(estimator=DTBinary, param_grid=param_grid1, cv= 5)\n",
    "CV_DT.fit(X4, y4)\n",
    "print CV_DT.best_params_\n",
    "print CV_DT.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_classDT = KFold(len(y4), shuffle=False) \n",
    "print cv_classDT\n",
    "DTBinary = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features='log2', max_leaf_nodes=None, min_samples_leaf=1,\n",
    "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')\n",
    "DTBinaryScore = cross_val_score(DTBinary, X4, y4, cv=cv_class,)\n",
    "print \"Decision Tree classifier scores are:\", DTBinaryScore\n",
    "print \"Decision Tree classifier average score is:\", DTBinaryScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DTBinary.fit(X4,y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DTBinary_pred = rfclassBi.predict(Class15Bi)\n",
    "DTBinary_predict = pd.DataFrame()\n",
    "DTBinary_predict['actual'] = Test['BiClass']\n",
    "DTBinary_predict['predict'] = DTBinary_pred\n",
    "DTBinary_probs = rfclassBi.predict_proba(Class15Bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_mat = pd.crosstab(DTBinary_predict['actual'], DTBinary_predict['predict'], rownames=['actual'])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision tree model is just as good as the random forest at predicting the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Bifpr = dict()\n",
    "Bitpr = dict()\n",
    "Biroc_auc = dict()\n",
    "Bifpr, Bitpr, _ = roc_curve(DTBinary_predict.actual, DTBinary_predict.predict)\n",
    "Biroc_auc = auc(Bifpr, Bitpr)\n",
    "\n",
    "# Plot of a ROC curve \n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(Bifpr, Bitpr, label='Rf AUC = %0.2f' % Biroc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate',fontsize=40)\n",
    "plt.ylabel('True Positive Rate', fontsize=40)\n",
    "plt.title('Receiver operating characteristic', fontsize=40)\n",
    "plt.legend(loc=\"lower right\", fontsize=40)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(DTBinary.fit(X4,y4), out_file='tree.dot', feature_names=X4.columns)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting exported dot file to png file using bash\n",
    "! dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recommendations and limitations\n",
    "\n",
    "We can use this model to predict whether a day will be a high or low sales day. We can then use this to decide where to spend our advertising budget. We can even look through the actual decision tree to make decisions based on the categorization of the features that we know.\n",
    "\n",
    "One major limitation is that we did not treat this as time series data. We can also play with thresholds for deciding whether or not a day will be a high or low sales day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "C5 Score: | 26/27\n",
    "------------|-----------\n",
    "Identify: Review executive summary, audience, goals, criteria\t|\t3\t\t\n",
    "Acquire: Review data selection & acquisition process\t\t\t|\t2\n",
    "Parse: Review data descriptions, outliers, risks, assumptions\t|\t3\t\t\n",
    "Mine: Review statistical analysis\t\t\t\t|2\n",
    "Refine: Review visual analysis\t\t\t\t|2\n",
    "Model: Review model and performance\t\t\t|2\t\n",
    "Present: Tell/sell the story to a non-tech audience |  3\n",
    "Present: Discuss findings and limitations\t|\t\t\t3\n",
    "Present: Create targeted recommendations and next steps\t\t|\t3\t\n",
    "Bonus: Deploy: Address how to (re)train model over time  | 2\n",
    "Bonus: Create an interactive demo of your data|  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
